## 搭建自己的神经网络 ##
----------
### 序言 ###
1. 智能机器探索
2. 新的黄金时代

### 绪论 ###
1. 本书针对哪些读者
2. 我们将做什么
3. 我们应该怎么做
4. 作者的温馨提示

### 第一部分：神经网络是怎样工作的 ###
1. 对我易，对你难
2. 一个简单的预测机
3. 分类不同于预测
4. 训练一个简单的分类
5. 有时一个分类是不够的
6. 神经元，大自然的计算机
7. 通过一个神经网络发出信号
8. 矩阵乘法是有用的
9. 一个三层神经网络的矩阵乘法的例子
10. 从多个节点学习权重
11. 来自多个输出节点的反向传播错误
12. 反向传播错误传递到更多层
13. 带有矩阵乘法的反向传播错误
14. 我们应该怎样更新权重
15. 更新权重的一个例子
16. 准备数据

### 第二部分：Python实现 ###
1. Python介绍
2. Interactive = Ipython
3. 一个简单的开始
4. 用Python实现神经网络
5. MNIST手写数据集

### 第三部分：更有趣的东西 ###
1. 你自己的手写数字
2. 神经网络的本质
3. 制作新的数据集：Rotations


### 后记 ###


### 第一部分：神经网络是怎样工作的 ###
**1. 对我易，对你难**
> 关键点：
> 
> 1.一些任务对传统的计算机是容易的，但是对人来说是困难的。例如：成千上万的数据对。
> 
> 2.另一方面，一些任务对传统的计算机来说是苦难的，但是对人来说是容易的。例如：在照片中识别人脸。

**2. 一个简单的预测机**
> 关键点：
> 
> 1.所用有用的计算机系统都有一个输入和输出，并且输入输出之间有计算过程，而神经网络是不同的。
> 
> 2.当我们不知道该做什么的时候，我们应该尝试着通过一个可以改变参数的模型去评估它。如果我们不知道怎样把千米转换为米，我们可以用线性函数作为一个模型，并且带一个合适的梯度。
> 
> 3.重新定义这些模型的一个好的方式是通过对比模型和正确例子的错误程度来改变模型参数。

**3. 分类不同于预测**

**4. 训练一个简单的分类**

初步理解机器学习中的学习率(Learning rate)
> 关键点：
> 
> 1.我们可以用简单的数学知识来理解一个线性分类器的输出误差和斜率之间的关系。这与我们怎样去调整斜率来减少输出误差是相同的。
> 
> 2.做这些参数调整的一个问题是：这个模型最后的更新仅仅是匹配到最后一个训练数据，并没有考虑之前所有的训练数据。一个好的做法是通过学习率来调整参数的更新，因此不再是一个训练数据决定这个学习。
> 
> 3.现实生活中的训练数据集可能包含噪声和错误数据，通过学习率进行参数的更新有利于限制错误数据带来的影响。

**5. 有时一个分类是不够的**

通过AND, OR和XOR引出线性分类器的不足，引出神经网络。对于一个线性分类器无法解决的问题有时需要多个线性分类器来解决。
> 关键点：
> 
> 1.一个简单的线性分类器不能够分离不被单一线性分类器管理的数据，例如通过逻辑操作符XOR产生的数据。
> 
> 2.解决办法是容易的，你可以通过多个线性分类器来划分数据，这些数据不能够被一个简单的线性分类器分开。

**6. 神经元，大自然的计算机**
> 关键点：
> 
> 1.尽管与现代计算机相比，生物大脑有很少的存储空间，并且运行速度很慢，但是它能够完成复杂的任务，例如飞行，寻找食物，学习语言和躲避掠食者。
> 
> 2.与传统的计算机系统相比，生物大脑对已经损伤和不完善的信号有难以置信的恢复能力。
> 
> 3.人工神经网络的灵感来自于连接神经元的生物大脑。

**7. 通过一个神经网络发出信号**

介绍了一个简单的两层神经网络的信号传递计算过程。

**8. 矩阵乘法是有用的**
> 关键点：
> 
> 1.神经网络的许多前馈信号计算操作可以表示为矩阵乘法。
> 
> 2.无论神经网络有多大，对于我们写来说，矩阵乘法使得前馈信号的表达更加精确。
> 
> 3.更重要的是，一些计算机编程语言能够很好地理解矩阵乘法，因此通过矩阵乘法计算更加快速有效。

**9. 一个三层神经网络的矩阵乘法的例子**

**10. 从多个节点学习权重**

**11. 来自多个输出节点的反向传播错误**

**12. 反向传播错误传递到更多层**
> 关键点：
> 
> 1.神经网络通过重新定义层与层之间的权重进行学习，误差=训练数据的正确值-实际输出值。
> 
> 2.输出节点上的误差是期望输出和实际输出之间的差值。
> 
> 3.然而，神经网络内部之间的误差是不明显的。一种方法是根据连接的权重对输出的误差按比例分配，在每一个内部节点上重新定义误差。

**13. 带有矩阵乘法的反向传播错误**
> 关键点：
> 
> 1.误差的反向传播可以通过矩阵乘法来表示。
> 
> 2.无论网络的大小是多少，都可以用矩阵乘法表示。计算机语言能够更有效地理解矩阵乘法。
> 
> 3.信号的前向传递和误差的反向传播使得用矩阵操作更有效。

**14. 我们应该怎样更新权重**
> 关键点：
> 
> 1.梯度下降是求函数最小值的一个很好的方法。当函数很复杂并且不能够用代数方法容易解决时，梯度下降仍然可以很好地解决。
> 
> 2.当函数带有很多参数时，其它的数学方法可能无法求解函数最小值，但梯度下降仍然可以很好地解决。
> 
> 3.该方法对数据中存在的错误也有一定的适应性。如果函数没有完全描述或者我们采取了错误的步骤，梯度下降也可以做出正确的决定。

> 关键点2：
> 
> 1.神经网络的误差是和权重相关的函数。
> 
> 2.改变神经网络的权重可以减小误差。
> 
> 3.直接选择正确的权重比较困难，正确的方法是通过一步步更新神经网络的权重来的减小神经网络的误差，这种方法是梯度下降(Gradient Descent)。
> 
> 4.通过微分操作求解误差斜率不是非常困难。

**15. 更新权重的一个例子**

**16. 准备数据**
> 关键点：
> 
> 1.如果神经网路的输入数据、输出数据和权重没有很好的根据神经网络的特点进行设置，神经网络将不能很好地工作。
> 
> 2.当一些输入信号带有比较大的权重值时，会导致激活函数的斜率变得平缓（趋于激活函数的饱和区域），这会减慢权重学习的能力。
> 
> 3.当输入信号值为0或者权重值为0时，将不能够学习到更好的权重。
> 
> 4.权重应该是随机分配并且较小，避免将权重设置为0。根据复杂的规则，当有更多的输入连接到一个节点时，可以适当的减少权重的值。
> 
> 5.神经网络的输入数据应该缩放到比较小的范围，但不应该是0。一个普遍的范围是0.01到0.99,或者-0.1到+0.1，实际应用中根据具体问题进行选择。
> 
> 6.神经网络输出值的范围应该在激活函数的域值范围内。小于0或者大于1的值不在Sigmod函数的域值范围内。如果神经网络输出的值不在有效的范围内，将会产生比较大的权重，导致饱和现象。一个好的范围时候0.01到0.99。
> 
### 第二部分：Pyhton实现 ###
**1. Pyhton介绍**

**2. Interactive = Ipython**

**3. 一个简单的开始**
> 通过一个简单的程序片段说明：


**4. 用Python实现神经网络**
> 讲述如何实现一个三层的神经网络。

**5. MNIST手写数据集**
> 1.介绍MNIST数据集；
> 
> 2.用三层神经网络识别手写数字。

### 第三部分：有趣的发现 ###
**1. 制作自己的手写数字集**

**2. 神经网络的黑盒问题**
> 详细叙述了神经网络的黑盒（black box）问题，并描述了如何通过输出output还原输入input，可以间接地理解神经网络最终学习到了什么。

**3. 旋转创造新的数据集**